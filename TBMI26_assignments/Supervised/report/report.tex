\documentclass{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{color}
\usepackage{url}
\usepackage{caption}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{%
    aboveskip=3mm, belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{red},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\val}[1]{#1_{\text{val}}}
\newcommand{\est}[1]{#1_{\text{est}}}

\begin{document}
\title{Lab 1 --- Fundamental Signal Processing}
\author{Malcolm Vigren, \textit{malvi108} \\
        Emil Segerbäck, \textit{emise935}}

\maketitle

Inställningar för 1:
numHidden = 8
numIterations = 1000
learningRate = 0.001

Inställningar för 2:
numHidden = 10
numIterations = 4000
learningRate = 0.002

Inställningar för 3:
numHidden = 16
numIterations = 1200
learningRate = 0.004

Inställningar för 4:
numHidden = 100
numIterations = 4000
learningRate = 0.004

\section{Overview}
Dataset 1 can be classified using a linear classifier. However, the other datasets are not
linearly separable.

\subsection{Downsampling}
Downsampling the data removes noise from the images which prevents the network from trying
to learn the noise.

\subsection{kNN}
We used the \texttt{mink} function in matlab to find the closest k data points and then we
use \texttt{mode} to check for the most frequent kind.

\subsection{Draws in kNN}
Because \texttt{mode} returns the smallest element in a case of a tie that will also
happen to our algorithm.

\subsection{Selection of best k}
We looped for all k between 1 and 100 and checked for which values it worked best.

\subsection{Backpropagation}
\subsubsection{Single layer}
\begin{equation}
  \frac{\partial \epsilon}{\partial w_{ij}} =
  \frac{\partial \epsilon}{\partial z_j}
  \frac{\partial z_j}{\partial s_j}
  \frac{\partial s_j}{\partial w_{ij}}
\end{equation}

where $s$ is

\begin{equation}
  s_j = \sum_{k}{w_{kj}h_k}
\end{equation}

and $z_j = \sigma(s_j)$

\subsubsection{Multi layer}
\begin{equation}
  \frac{\partial \epsilon}{\partial w_{ij}} =
  \frac{\partial \epsilon}{\partial z_j}
  \frac{\partial z_j}{\partial s_j}
  \frac{\partial s_j}{\partial w_{ij}}
\end{equation}

where $s$ is

\begin{equation}
  s_j = \sum_{k}{w_{kj}h_k}
\end{equation}

and $z_j = \sigma(s_j)$

\subsection{Training}

\subsection{Solution}

\subsection{Discussion}

\subsection{Improvements}

\end{document}
